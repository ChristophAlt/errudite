{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Preprocessing\n",
    "\n",
    "This section discusses how to transfer raw data and raw models into the format required by Errudite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Must-do setup\n",
    "\n",
    "\n",
    "**!! VERY IMPORTANT**\n",
    "\n",
    "To succesfully run this notebook and all the following notebooks, make sure you install required dependencies:\n",
    "\n",
    "```sh\n",
    "# first, start you virtual environment.\n",
    "# Assuming you are in the top errudite folder.\n",
    "cd tutorial/\n",
    "pip install requirements_tutorial.txt\n",
    "```\n",
    "\n",
    "These will make sure you can use dependencies that are not in the main package requirements. For example, here we load predictors from `Allennlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def import_sys():\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "import_sys()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'errudite' from '../errudite/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import errudite\n",
    "print(errudite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Preview\n",
    "We use Textual Entailment (TE) task as a demo task here. TE models take a pair of sentences and predict whether the facts in the first necessarily imply the facts in the second one. The dataset we have here is 100 instances from the [Stanford Natural Language Inference (SNLI)](https://nlp.stanford.edu/projects/snli/), a large annotated for learning natural language inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Input\n",
    "\n",
    "For data grouping and rewriting to work, Errudite requires the input raw texts to be parsed and annotated with LEMMAs, POS tags, named entities, and parsing tree structures. The preprocessing is based on [SpaCy@2](https://spacy.io/). This notebook shows how to transfer raw text inputs into annotated instance objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a `DatasetReader`.\n",
    "---\n",
    "\n",
    "A ``DatasetReader`` ([documentation here](https://errudite.readthedocs.io/en/latest/api/_extensible_dataset_reader.html)) loads the raw data from a data file, preprocess the data to include linguistic features, and save the processed data to a cache folder. All the task specific readers are *registered* under the base class ``DatasetReader``, so they could be queried via their names:\n",
    "\n",
    "```python\n",
    "DatasetReader.by_name(\"dataset_name\")\n",
    "```\n",
    "\n",
    "Because the reader also handles dumping the processed instances into cache files, we require you to provide a desired cache path. If not provided, the default path is `./caches/`\n",
    "\n",
    "To get the pre-implemented reader for SNLI, we could run the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.utils.file_utils:Errudite cache folder selected: ./data/snli_tutorial_cache/\n"
     ]
    }
   ],
   "source": [
    "from errudite.io import DatasetReader\n",
    "\n",
    "cache_folder_path = \"./data/snli_tutorial_caches/\"\n",
    "reader = DatasetReader.by_name(\"snli\")(cache_folder_path=cache_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Transfer the raw data to `Instance`s and `Target`s\n",
    "---\n",
    "\n",
    "With the reader, the first thing we do is to read from the raw dataset file, and transfer them into `Instance`s, a data structure used by Errudite, which contains various `Target`s. See more details in [this documentation](https://errudite.readthedocs.io/en/latest/api/_targets_and_instances.html).\n",
    "\n",
    "### Raw data structure\n",
    "\n",
    "In SNLI, one raw data takes the following structure:\n",
    "\n",
    "\n",
    "```python\n",
    "# A unique id denoting each sentence pair. \n",
    "pairID                                                  \"4705552913.jpg#2r1n\"\n",
    "# A more general id that groups relevant sentence pairs.\n",
    "captionID                                                  \"4705552913.jpg#2\"\n",
    "# The premise caption that was supplied to the author of the pair.\n",
    "sentence1                 \"Two women are embracing while holding to go pa...\"\n",
    "# The hypothesis caption that was written by the author of the pair.\n",
    "sentence2                 \"The sisters are hugging goodbye while holding ...\"\n",
    "#  This is the label chosen by the majority of annotators.\n",
    "gold_label                                                          \"neutral\"\n",
    "# the majority of the following form the gold_label.\n",
    "label1                                                              \"neutral\"\n",
    "label2                                                           \"entailment\"\n",
    "label3                                                              \"neutral\"\n",
    "label4                                                              \"neutral\"\n",
    "label5                                                              \"neutral\"\n",
    "# These are default parsing information that we do not use.\n",
    "sentence1_parse           \"(ROOT (S (NP (CD Two) (NNS women)) (VP (VBP ar...\"\n",
    "sentence2_parse           \"(ROOT (S (NP (DT The) (NNS sisters)) (VP (VBP ...\"\n",
    "sentence1_binary_parse    \"( ( Two women ) ( ( are ( embracing ( while ( ...\"\n",
    "sentence2_binary_parse    \"( ( The sisters ) ( ( are ( ( hugging goodbye ...\"\n",
    "```\n",
    "\n",
    "\n",
    "### Processed `Instance`, and their associated variables\n",
    "\n",
    "Each raw data above is transfered into an `Intance`, with the following information: \n",
    "\n",
    "#### IDs\n",
    "An instance is identified (hashed) with:\n",
    "- `qid`: unique identifier or id. (`pairID` in example, a unique identifier for each sentence1--sentence2 pair.)\n",
    "- `vid`: Notes the version of a target. The original inputs are version 0. When a target is rewritten, vid increases. \n",
    "\n",
    "#### Targets\n",
    "\n",
    "Targets are primitives that allow users to access inputs and outputs at different levels of granularity. What's essential in all the analysis are these `Target`s -- In fact, you could treat an `Instance` as a wrapper for targets.\n",
    "\n",
    "In this task, we transfer `sentence1` (`premise`), `sentence2` (`hypothesis`), and `gold_label` (`groundtruth`) into targets. In addition, though not in the loaded dataframe above, each instance has predictions from models -- We will generate them as we go on. \n",
    "\n",
    "To do the transfer, call the `read()` function in the reader, and see the first instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.io.dataset_reader:Reading instances from lines in file at: data/snli_dev_100.txt\n",
      "INFO:errudite.io.snli_reader:Reading instances from lines in file at: data/snli_dev_100.txt\n",
      "100it [00:05, 17.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# read the raw data!\n",
    "instances = reader.read(\n",
    "    # The path of the input data file. We are using the first 100 rows from the SNLI dev set.\n",
    "    file_path='data/snli_dev_100.txt', \n",
    "    # If sample size is set, only load this many of instances, by default None.\n",
    "    sample_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on `Target` API\n",
    "\n",
    "(Or: detailed steps in that `reader()` step.)\n",
    "\n",
    "We define a general `Target` class which takes **four** inputs:\n",
    "- `text`: The raw text will be processed with spacy\n",
    "- `qid`: The id of the instance\n",
    "- `vid`: The version\n",
    "- `metas`: Sometimes a specific target can take additional inputs\n",
    "\n",
    "For example, our `SNLIReader` uses the following lines to create `hypothesis` and `premise`:\n",
    "```python\n",
    "from errudite.targets.target import Target\n",
    "premise = Target(\n",
    "    qid='4705552913.jpg#2r1n', \n",
    "    text=\"The sisters are hugging goodbye while holding to go packages after just eating lunch.\", vid=0, metas={'type': 'premise'})\n",
    "hypothesis = Target(\n",
    "    qid='4705552913.jpg#2r1n', \n",
    "    text='Two women are embracing while holding to go packages.', \n",
    "    vid=0, \n",
    "    metas={'type': 'hypothesis'})\n",
    "print(premise)\n",
    "print(hypothesis)\n",
    "```\n",
    "\n",
    "####  Special case of `Target`: `Label`\n",
    "\n",
    "`Label` is a special subclass of Target, denoting _groundtruth_ and _prediction_. It takes an additional input:\n",
    "- `model`: Which predictor the label is producied by. For groundtruths, make sure this model is `groundtruth`.\n",
    "\n",
    "Because `Label` can be of different types (`int`, predefined class `str`, or span `str` extracted from certain targets), we define two subclasses of `Label`.\n",
    "- `SpanLabel`: To handle tasks like QA, where the output label is a sequence span extracted from input (context), and therefore is not a predefined set. These labels are similarly processed by SpaCy to be queryable.\n",
    "- `PredefinedLabel`: To handle tasks where the output label are discrete, predefined class types. These outputs will not need any preprocessing.\n",
    "\n",
    "Here, the groundtruth label is always one of `['neutral', 'contradiction', 'entailment']`. Therefore, we define it with `PredefinedLabel`:\n",
    "\n",
    "```python\n",
    "from errudite.targets.label import PredefinedLabel\n",
    "raw_labels = [row[f'label{i}']  for i in range(1,6)]\n",
    "groundtruth = PredefinedLabel(\n",
    "    model='groundtruth', \n",
    "    qid='4705552913.jpg#2r1n', \n",
    "    text='neutral', \n",
    "    vid=0, \n",
    "    # we can save the raw labels into the groundtruth as well:\n",
    "    metas={'raw_labels': ['neutral', 'neutral', 'neutral', 'neutral', 'entailment']}\n",
    ")\n",
    "```\n",
    "\n",
    "#### Merging targets into instances\n",
    "instance\n",
    "Create  classes by setting the correct entries and keys created by the targets. This becomes the wrapper class which is used by the DSL to create specific instances. \n",
    "\n",
    "**!!** While other entires can flow, make sure you set [`predictions` or `prediction`] and [`groundtruths` or `groundtruth`], depending on how many groundtruths you have, and how many models you are using to predict this one instance.\n",
    "All of them are saved into the instance:\n",
    "\n",
    "```python\n",
    "instance.set_entries(\n",
    "    hypothesis=hypothesis, \n",
    "    premise=premise, \n",
    "    groundtruth=groundtruth)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load models & add the missing `prediction` target\n",
    "---\n",
    "\n",
    "Like mentioned before, though not included in the input each instance can have a prediction from a model (Or prediction**s** from multiple model**s** if you plan to do model comparison.)\n",
    "\n",
    "Predictions can be loaded from files, just like how you would create groundtruth targets with `Label` classes. Alternatively, you can **get predictions from actual predictors in real time.**\n",
    "\n",
    "This is especially important if you plan to do any form of rewriting later -- You can only test a rewrite if you have a model that can re-run predictions on the newly created, rewritten instances!\n",
    "\n",
    "This part shows you how to load a `Predictor` (more in the [documentation](https://errudite.readthedocs.io/en/latest/api/_extensible_predictor.html)!)\n",
    "\n",
    "\n",
    "### Getting the predictions\n",
    "\n",
    "The basic `Predictor` class wraps the following information:\n",
    "1. `name`: An identifier of the predictor.\n",
    "2. `description`: A description for you to remember you model\n",
    "3. `predictor`: The actual trained model\n",
    "3. `perform_metrics`: metrics you will want to evaluate your model on\n",
    "\n",
    "Below, we create a `predictor` with the allennlp pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from errudite.predictors import Predictor\n",
    "model_online_path = \"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\"\n",
    "predictor = Predictor.by_name(\"nli_decompose_att\")(\n",
    "    name='decompose_att', \n",
    "    description='Pretrained model from Allennlp, for the decomposable attention model',\n",
    "    model_online_path=model_online_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictor has two types of predictions: \n",
    "1. A raw prediction function, which takes texts, and return a json format prediction output:\n",
    "\n",
    "```python\n",
    "# individual model prediction\n",
    "predictor.predict(\n",
    "  hypothesis=hypothesis.get_text(),\n",
    "  premise=premise.get_text()\n",
    ")\n",
    "# returns: {'confidence': 0.9988018274307251, 'text': 'neutral'}\n",
    "```\n",
    "\n",
    "2. A class method that takes Target inputs, run model predictions, and wrap the output prediction into Labels.\n",
    "\n",
    "```python\n",
    "# individual model prediction\n",
    "prediction = Predictor.by_name(\"nli_task_class\".model_predict(\n",
    "    predictor=predictor, \n",
    "    premise=premise, \n",
    "    hypothesis=hypothesis, \n",
    "    groundtruth=groundtruth)\n",
    "# returns: \n",
    "# [PredefinedLabel] [LabelKey(qid='4705552913.jpg#2r1n', vid=0, model='decompose_att', label='neutral')]\n",
    "# neutral ({'accuracy': 1.0, 'confidence': 0.9988018274307251})\n",
    "```\n",
    "\n",
    "We run it on every instance, and save them as `predictions` entries into the instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Running predictions....\n",
      "100%|██████████| 100/100 [01:29<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "logger.info(\"Running predictions....\")\n",
    "for instance in tqdm(instances):\n",
    "    prediction = Predictor.by_name(\"nli_task_class\").model_predict(\n",
    "        predictor, \n",
    "        premise=instance.premise, \n",
    "        hypothesis=instance.hypothesis, \n",
    "        groundtruth=instance.groundtruth)\n",
    "    # set the prediction\n",
    "    instance.set_entries(predictions=[ prediction ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The completed instances have several functions that helps you to query targets or performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Target] [InstanceKey(qid='4705552913.jpg#2r1n', vid=0)]\n",
      "Two women are embracing while holding to go packages. \n",
      "\n",
      "False \n",
      "\n",
      "[Instance] [InstanceKey(qid='4444092165.jpg#0r1n', vid=0)]\n",
      "[hypothesis]\tThe man is getting ready to speak.\n",
      "[premise]\tA white-haired man with a mustache and glasses in a business suit stands outside at a podium marked with the seal of the US House of Representatives, surrounded by many people, with a columned building behind him.\n",
      "[groundtruth]\tneutral\tgroundtruth\t{}\n",
      "[predictions]\tneutral\tdecompose_att\t{'accuracy': 1.0, 'confidence': 0.7993758916854858}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(instances[0].get_entry('premise'), \"\\n\")\n",
    "print(instances[0].is_incorrect(model='decompose_att'), \"\\n\")\n",
    "\n",
    "instance.show_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the predictions generated, we can use them to compute the model's overall performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor': 'decompose_att', 'perform': {'accuracy': 0.92, 'confidence': 0.8930190017819405}}\n"
     ]
    }
   ],
   "source": [
    "predictor.evaluate_performance(instances)\n",
    "print({\"predictor\": predictor.name, \"perform\": predictor.perform })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save them to the hashes attached to `Instance`, which will build three hashes:\n",
    "\n",
    "1. `Instance.instance_hash`: `Dict[InstanceKey, Instance]`, A dict that saves all the original instances, denoted by the corresponding instance keys.\n",
    "2. `Instance.instance_hash_rewritten`: `Dict[InstanceKey, Instance]`, A dict that saves all the rewritten instances, denoted by the corresponding instance keys.\n",
    "3. `Instance.qid_hash`: `Dict[str, List[InstanceKey]]`, A dict that denotes wraps different versions of instance keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------\n",
    "# Build the instance store hash\n",
    "from errudite.targets.instance import Instance\n",
    "instance_hash, instance_hash_rewritten, qid_hash = Instance.build_instance_hashes(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute related distributions for further analysis\n",
    "---\n",
    "\n",
    "Besides creating instances, the `reader` preprocesses and computes two more things:\n",
    "\n",
    "1. Compute the vocabulary from a given data file. This is for getting the training frequency and save to `Instance.train_freq` in the format of:\n",
    "```python\n",
    "{ vocab[str] : count[int] }\n",
    "```\n",
    "\n",
    "2. We Compute the relationship between linguistic features and model performances. It’s used for the programming by demonstration. The result is saved to `Instance.ling_perform_dict`. It’s in the format of:\n",
    "\n",
    "```python\n",
    "{\n",
    "    target_name[str] : { # e.g. \"premise\"\n",
    "        pattern[str]: { # e.g. \"two women are\", \"two NOUN are\"\n",
    "            model_name[str]: { # e.g. \"decompose_att\"\n",
    "                \"cover\": # how many instances are there.\n",
    "                \"err_cover\": # The ratio of incorrect predictions with the pattern, \n",
    "                             # overall all the incorrect predictions.\n",
    "                \"err_rate\":  # the ratio of incorrect predictions, \n",
    "                             # over all the instances wit the pattern.\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:errudite.utils.file_utils:Local path not yet exist, but still parsed: /Users/tongshuangwu/sourcetree/errudite/tutorials/data/caches/snli_tutorial/vocab.pkl\n",
      "WARNING:errudite.processor.spacy_annotator:(2, 'No such file or directory')\n",
      "INFO:errudite.io.dataset_reader:Computing vocab frequency from file at: data/snli_train_1000.txt\n",
      "INFO:errudite.io.snli_reader:Reading instances from lines in file at: data/snli_train_1000.txt\n",
      "999it [00:00, 1777.90it/s]\n",
      "INFO:errudite.io.dataset_reader:Computing premise frequency.\n",
      "INFO:errudite.io.dataset_reader:Computing hypoethsis frequency.\n",
      "INFO:errudite.io.dataset_reader:Computing linguistic performance distribution per instance...\n",
      "100%|██████████| 100/100 [00:16<00:00,  5.42it/s]\n",
      "INFO:errudite.io.dataset_reader:Computing the final distribution...\n"
     ]
    }
   ],
   "source": [
    "# ---------\n",
    "# Compute the vocabulary from the given training data file.\n",
    "reader.count_vocab_freq('data/snli_train_1000.txt')\n",
    "# ---------\n",
    "# Compute the relationship between linguistic features and model performances.\n",
    "reader.compute_ling_perform_dict(list(Instance.instance_hash.values()))\n",
    "# ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Save your processed data\n",
    "\n",
    "It becomes tedious to re-run the preprocessing step again and again. \n",
    "`Instance.dump_preprocessed()` saves all the preprocessed information to the cache file. It generates the following files in the cache folder:\n",
    "\n",
    "```sh\n",
    "[cache folder]\n",
    "│   # saved attr, group and rewrite json that can be reloaded. \n",
    "│   # we talk about these in the next tutorial.\n",
    "├── analysis \n",
    "├── evaluations # predictions saved by the different models, with the model name being the folder name.\n",
    "│   └── bidaf.pkl\n",
    "├── instances.pkl # Save all the `Instance`, with the processed Target.\n",
    "│   # A dict saving the relationship between linguistic features and model performances. \n",
    "│   # It's used for the programming by demonstration.\n",
    "├── ling_perform_dict.pkl\n",
    "├── train_freq.json # The training vocabulary frequency\n",
    "└── vocab.pkl # The SpaCy vocab information.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:errudite.io.dataset_reader:Dumped 100 objects to ./data/caches/snli_tutorial/instances.pkl.\n",
      "INFO:errudite.io.dataset_reader:Dumped 100 objects to ./data/caches/snli_tutorial/evaluations/decompose_att.pkl.\n",
      "INFO:errudite.io.dataset_reader:Dumped the linginguistic perform dict.\n"
     ]
    }
   ],
   "source": [
    "reader.dump_preprocessed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend Errudite for your own task\n",
    "---\n",
    "\n",
    "To extend Errudite to your own task and model, you will need to write your own `DatasetReader`, and your own `Predictor` wrapper. As we've seen, a `DatasetReader` knows how to turn a file containing a dataset into a collection of Instance s, and how to handle writting the processed instance caches to the cache folders. A `Predictor` wraps the prediction function of a model, and transfers the prediction to Label targets.\n",
    "\n",
    "This section shows you how to implement both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `DatasetReader`\n",
    "\n",
    "Your could define your own dataset reader by registering a task-specific reader under the abstract ``DatasetReader`` class -- Just make sure you override the `self._read` and `self._text_to_instance`. \n",
    "\n",
    "To give you a taste of how to do the implementation, we copy-paste an implementation for loading  [Stanford Natural Language Inference (SNLI)](https://nlp.stanford.edu/projects/snli/) that offered by default in ``errudite.io.snli_reader.SNLIReader`` (See the comments for implementation tips):\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from errudite.io import DatasetReader\n",
    "from errudite.utils import normalize_file_path, accuracy_score\n",
    "from errudite.targets.instance import Instance\n",
    "from errudite.targets.target import Target\n",
    "from errudite.targets.label import Label, PredefinedLabel\n",
    "\n",
    "@DatasetReader.register(\"snli\")\n",
    "class SNLIReader(DatasetReader):\n",
    "    def __init__(self, cache_folder_path: str=None) -> None:\n",
    "        super().__init__(cache_folder_path)\n",
    "        # overwrite the primary evaluation method and metric name\n",
    "        Label.set_task_evaluator(accuracy_score, 'accuracy')\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str, lazy: bool, sample_size: int):\n",
    "        \"\"\"\n",
    "        Returns a list containing all the instances in the specified dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : str\n",
    "            The path of the input data file.\n",
    "        lazy : bool, optional\n",
    "            If ``lazy==True``, only run the tokenization, does not compute the linguistic\n",
    "            features like POS, NER. By default False\n",
    "        sample_size : int, optional\n",
    "            If sample size is set, only load this many of instances, by default None\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        List[Instance]\n",
    "            The instance list.\n",
    "        \"\"\"\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        df = pd.read_csv(normalize_file_path(file_path), sep='\\t')\n",
    "        for idx, row in tqdm(df.iterrows()):\n",
    "            if lazy:\n",
    "                premises.append(row['sentence1'])\n",
    "                hypotheses.append(row['sentence2'])\n",
    "            else:\n",
    "                instance = self._text_to_instance(f'q:{idx}', row)\n",
    "                if instance is not None:\n",
    "                    instances.append(instance)\n",
    "                if sample_size and idx > sample_size:\n",
    "                    break\n",
    "        if lazy:\n",
    "            return { \"premise\": premises, \"hypoethsis\": hypotheses }\n",
    "        else:\n",
    "            return instances\n",
    "    \n",
    "    @overrides\n",
    "    def _text_to_instance(self, id: str, row) -> Instance:\n",
    "        # The function that transfers raw text to instance.\n",
    "        premise = Target(qid=row['pairID'], text=row['sentence1'], vid=0, metas={'type': 'premise'})\n",
    "        hypothesis = Target(qid=row['pairID'], text=row['sentence2'], vid=0, metas={'type': 'hypothesis'})\n",
    "        # label\n",
    "        raw_labels = [row[f'label{i}']  for i in range(1,6)]\n",
    "        groundtruth = PredefinedLabel(\n",
    "            model='groundtruth', \n",
    "            qid=row['pairID'], \n",
    "            text=row['gold_label'], \n",
    "            vid=0, \n",
    "            metas={'raw_labels': raw_labels}\n",
    "        )\n",
    "        return self.create_instance(row['pairID'], \n",
    "            hypothesis=hypothesis, \n",
    "            premise=premise, \n",
    "            groundtruth=groundtruth)\n",
    "```\n",
    "\n",
    "This reader, as we did before, can be queried via:\n",
    "```python\n",
    "from errudite.readers import DatasetReader\n",
    "DatasetReader.by_name(\"snli\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `Predictor`\n",
    "\n",
    "To use your own predictors / models, you need to extend the predictor class to do three things:\n",
    "1. Define a list of `perform_metrics`, or metrics you will want to evaluate your model on. You can the task evaluator setter in you predictor `__init__()`. This includes two sub-steps:\n",
    "    - First, define the evaluation function to determine how well a model is doing on one instance, based on an individual predicted label.\n",
    "    - Second, from the metrics above, pick one that's primary, and it will be used to compute `is_incorrect()` in any label target object: primary metric < 1.\n",
    "2. Define a class method `model_predict()` that takes `Target` inputs, run model predictions, and wrap the output prediction into Labels.\n",
    "3. Wrap your raw model prediction:\n",
    "    - Save your model as a variable (`model` variable in a `Predictor` object)\n",
    "    - Wrap your mode prediction method in a `predict()` method    \n",
    "    \n",
    "To make your life easier, we've already created `perform_metrics` and the classmethod `model_predict()` for VQA, QA, Sentiment Analysis, and NLI tasks. See the corresponding folders under `errudite/predictors/`. They are also examples for you to extend for your own tasks. We copy-paste the `errudite.predictors.nli.predictor_decompose_att` to give you a taste of the actual implementation, which can be queried (as we did before):\n",
    "\n",
    "```python\n",
    "from errudite.predictors import Predictor\n",
    "Predictor.by_name(\"nli_decompose_att\")\n",
    "```\n",
    "\n",
    "The implementation:\n",
    "```python\n",
    "from typing import List, Dict\n",
    "from ..predictor import Predictor\n",
    "from ...targets.label import Label, PredefinedLabel\n",
    "from ..predictor_allennlp import PredictorAllennlp # a wrapper for Allennlp classes\n",
    "\n",
    "@Predictor.register(\"nli_decompose_att\")\n",
    "class PredictorNLI(Predictor, PredictorAllennlp):\n",
    "    \"\"\"\n",
    "    The wrapper for DecomposableAttention model, as implemented in Allennlp:\n",
    "    https://allenai.github.io/allennlp-docs/api/allennlp.predictors.html#decomposable-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str, \n",
    "        model_path: str=None,\n",
    "        model_online_path: str=None,\n",
    "        description: str='') -> None:\n",
    "        PredictorAllennlp.__init__(self, name, model_path, model_online_path, description)\n",
    "        Predictor.__init__(self, name, description, model, perform_metrics)\n",
    "        # set the perform metrics\n",
    "        perform_metrics = ['accuracy', 'confidence']\n",
    "        # First, define the evaluation function to determine how well a model is doing \n",
    "        # on one instance, based on an individual predicted label.\n",
    "        from ...utils.evaluator import accuracy_score\n",
    "        # Second, from the metrics above, pick one that's primary, and it will be used \n",
    "        # to compute `is_incorrect()` in any label target object: primary metric < 1.\n",
    "        Label.set_task_evaluator(\n",
    "            # the evaluation function that accepts pred and groundtruths, \n",
    "            # and return a dict of metrics: { metric_name: metric_score }. \n",
    "            # This is saved as Label.task_evaluation_func.\n",
    "            task_evaluation_func=accuracy_score, \n",
    "            # The primary task metric name, ideally a key of task_evaluation_func ‘s return.\n",
    "            task_primary_metric='accuracy')\n",
    "\n",
    "    # the raw prediction function, returning the output of the model in a json format.\n",
    "    def predict(self, premise: str, hypothesis: str) -> Dict[str, float]:\n",
    "        try:\n",
    "            labels = ['entailment', 'contradiction', 'neutral']\n",
    "            predicted = self.model.predict_json({\n",
    "                \"premise\": premise, \"hypothesis\":hypothesis})\n",
    "            return {\n",
    "                'confidence': max(predicted['label_probs']),\n",
    "                'text': labels[np.argmax(label_probs)],\n",
    "            }\n",
    "        except:\n",
    "            raise\n",
    "\n",
    "    @classmethod\n",
    "    # the class method that takes `Target` inputs, and output a `Label` object.\n",
    "    def model_predict(cls, \n",
    "        predictor: Predictor, \n",
    "        premise: Target, \n",
    "        hypothesis: Target, \n",
    "        groundtruth: Label) -> 'Label':\n",
    "        answer = None\n",
    "        if not predictor:\n",
    "            return answer\n",
    "        predicted = predictor.predict(premise.get_text(), hypothesis.get_text())\n",
    "        if not predicted:\n",
    "            return None\n",
    "        answer = PredefinedLabel(\n",
    "            model=predictor.name, \n",
    "            qid=premise.qid,\n",
    "            text=predicted['text'], \n",
    "            vid=max([premise.vid, hypothesis.vid, groundtruth.vid] ))\n",
    "        answer.compute_perform(groundtruths=groundtruth)\n",
    "        answer.set_perform(confidence=predicted['confidence'])\n",
    "        return answer\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
